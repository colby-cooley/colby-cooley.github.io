[
  {
    "objectID": "traffic_stops.html",
    "href": "traffic_stops.html",
    "title": "Traffic Stops",
    "section": "",
    "text": "Traffic stop data offer important insights into how policing practices vary across time and demographic groups. In this project, I investigate whether search behavior varies by time of day and driver race across three distinct jurisdictions in the Stanford Open Policing Project database: Arizona Statewide, Oakland (CA), and Saint Paul (MN).\n\ncon_traffic &lt;- DBI::dbConnect(\n\n  RMariaDB::MariaDB(),\n\n  dbname = \"traffic\",\n\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n\n)\n\nTo address this question, I will attempt to combine three separate SQL tables into a unified dataset. I will then try to visualize the resulting search patterns to explore whether certain racial groups experience higher search rates during specific hours of the day.\nThe following table combines the data from all three tables into one, big table. It groups the data by the hour of the day and by the race of the driver. It also gives the number of stops, number of searches, and percentage of stops that were searches for each of these groups.\n\nSELECT\n  hour,\n  subject_race,\n  n_stops,\n  n_searched,\n  ROUND(100.0 * n_searched / n_stops, 2) AS pct_searched\nFROM (\n    SELECT\n      CONVERT(TIME_FORMAT(time, '%H'), UNSIGNED) AS hour,\n      subject_race,\n      COUNT(*) AS n_stops,\n      SUM(search_conducted) AS n_searched\n    FROM (\n        SELECT time, subject_race, search_conducted\n        FROM az_statewide_2020_04_01\n\n        UNION ALL\n\n        SELECT time, subject_race, search_conducted\n        FROM ca_oakland_2020_04_01\n\n        UNION ALL\n\n        SELECT time, subject_race, search_conducted\n        FROM mn_saint_paul_2020_04_01\n    ) AS combined\n    WHERE subject_race IS NOT NULL\n      AND TIME_FORMAT(time, '%H') IS NOT NULL\n    GROUP BY hour, subject_race\n    HAVING COUNT(*) &gt; 1000\n) AS hourly\nORDER BY hour, subject_race\n\n\nhourly\n\n    hour           subject_race n_stops n_searched pct_searched\n1      0 asian/pacific islander    6385        570         8.93\n2      0                  black   27994       4821        17.22\n3      0               hispanic   16906       2715        16.06\n4      0                  other    2317        387        16.70\n5      0                  white   41049       4055         9.88\n6      1 asian/pacific islander    4250        364         8.56\n7      1                  black   19170       3481        18.16\n8      1               hispanic   14072       2628        18.68\n9      1                  other    1444        315        21.81\n10     1                  white   28649       3551        12.39\n11     2 asian/pacific islander    2571        276        10.74\n12     2                  black   13260       2701        20.37\n13     2               hispanic   11386       2818        24.75\n14     2                  other    1051        389        37.01\n15     2                  white   19285       3152        16.34\n16     3 asian/pacific islander    1719        195        11.34\n17     3                  black    7927       1684        21.24\n18     3               hispanic    7866       1635        20.79\n19     3                  white   12840       1745        13.59\n20     4 asian/pacific islander    1217         97         7.97\n21     4                  black    5032        958        19.04\n22     4               hispanic    6912       1034        14.96\n23     4                  white   11533       1133         9.82\n24     5 asian/pacific islander    1601         76         4.75\n25     5                  black    6159        741        12.03\n26     5               hispanic   16769       1590         9.48\n27     5                  other    1432        227        15.85\n28     5                  white   33432       1334         3.99\n29     6 asian/pacific islander    2758         76         2.76\n30     6                  black   10886        898         8.25\n31     6               hispanic   38226       2667         6.98\n32     6                  other    4194        352         8.39\n33     6                  white   74716       2308         3.09\n34     7 asian/pacific islander    5521        150         2.72\n35     7                  black   17555       1532         8.73\n36     7               hispanic   56336       4036         7.16\n37     7                  other    7859        468         5.95\n38     7                unknown    1357         50         3.68\n39     7                  white  124248       3529         2.84\n40     8 asian/pacific islander    7507        211         2.81\n41     8                  black   21580       1869         8.66\n42     8               hispanic   63703       4323         6.79\n43     8                  other    9617        680         7.07\n44     8                unknown    1543         63         4.08\n45     8                  white  143734       4124         2.87\n46     9 asian/pacific islander    7536        202         2.68\n47     9                  black   19891       2039        10.25\n48     9               hispanic   59863       4185         6.99\n49     9                  other    9860        773         7.84\n50     9                unknown    1380         42         3.04\n51     9                  white  133696       3973         2.97\n52    10 asian/pacific islander    7779        218         2.80\n53    10                  black   20147       2227        11.05\n54    10               hispanic   57611       3976         6.90\n55    10                  other   10284        791         7.69\n56    10                unknown    1291         40         3.10\n57    10                  white  132448       4128         3.12\n58    11 asian/pacific islander    6721        196         2.92\n59    11                  black   16963       2037        12.01\n60    11               hispanic   47224       3504         7.42\n61    11                  other    8931        779         8.72\n62    11                unknown    1102         42         3.81\n63    11                  white  113014       3605         3.19\n64    12 asian/pacific islander    6676        229         3.43\n65    12                  black   18316       2149        11.73\n66    12               hispanic   52933       3823         7.22\n67    12                  other    9751        835         8.56\n68    12                unknown    1273         54         4.24\n69    12                  white  126468       4341         3.43\n70    13 asian/pacific islander    7365        223         3.03\n71    13                  black   21147       2747        12.99\n72    13               hispanic   62155       4626         7.44\n73    13                  other   11926       1159         9.72\n74    13                unknown    1531         70         4.57\n75    13                  white  143444       4816         3.36\n76    14 asian/pacific islander    7897        236         2.99\n77    14                  black   23185       3582        15.45\n78    14               hispanic   70806       5859         8.27\n79    14                  other   12548       1289        10.27\n80    14                unknown    1608         71         4.42\n81    14                  white  156751       5690         3.63\n82    15 asian/pacific islander    7194        279         3.88\n83    15                  black   22490       4147        18.44\n84    15               hispanic   66816       6449         9.65\n85    15                  other   12741       1461        11.47\n86    15                unknown    1536         81         5.27\n87    15                  white  145329       6001         4.13\n88    16 asian/pacific islander    7582        306         4.04\n89    16                  black   24024       4463        18.58\n90    16               hispanic   63707       6231         9.78\n91    16                  other   12567       1265        10.07\n92    16                unknown    1537         50         3.25\n93    16                  white  142870       5966         4.18\n94    17 asian/pacific islander    9036        373         4.13\n95    17                  black   28181       5031        17.85\n96    17               hispanic   59820       5641         9.43\n97    17                  other   12855       1282         9.97\n98    17                unknown    1398         51         3.65\n99    17                  white  142083       6480         4.56\n100   18 asian/pacific islander    7790        393         5.04\n101   18                  black   25261       4611        18.25\n102   18               hispanic   49482       4996        10.10\n103   18                  other   10163        955         9.40\n104   18                unknown    1116         51         4.57\n105   18                  white  119589       5530         4.62\n106   19 asian/pacific islander    7051        369         5.23\n107   19                  black   23903       4271        17.87\n108   19               hispanic   41610       4078         9.80\n109   19                  other    9049        908        10.03\n110   19                  white   99311       4762         4.80\n111   20 asian/pacific islander    7355        318         4.32\n112   20                  black   24939       3836        15.38\n113   20               hispanic   44177       4327         9.79\n114   20                  other    8755        988        11.28\n115   20                unknown    1037         47         4.53\n116   20                  white  105488       5080         4.82\n117   21 asian/pacific islander    7882        329         4.17\n118   21                  black   26847       3585        13.35\n119   21               hispanic   46507       4884        10.50\n120   21                  other    8366       1040        12.43\n121   21                unknown    1092         65         5.95\n122   21                  white  108378       5614         5.18\n123   22 asian/pacific islander    7797        392         5.03\n124   22                  black   29762       4305        14.46\n125   22               hispanic   40249       4404        10.94\n126   22                  other    6240        798        12.79\n127   22                  white   91427       5469         5.98\n128   23 asian/pacific islander    8575        521         6.08\n129   23                  black   35773       5410        15.12\n130   23               hispanic   30325       3941        13.00\n131   23                  other    4029        573        14.22\n132   23                  white   70334       5283         7.51\n\n\nThe following plot shows clear differences in stop frequency across hours of the day. Traffic stops increase substantially during daylight hours and decline late at night. Racial groups follow broadly similar patterns, though some groups appear to have disproportionately more stops during certain hours.\n\nlibrary(tidyverse)\n\nggplot(hourly, aes(x = hour, y = n_stops, color = subject_race)) +\ngeom_line(size = 1) +\nlabs(\ntitle = \"Number of Traffic Stops by Hour and Race\",\nx = \"Hour of Day (0–23)\",\ny = \"Number of Stops\",\ncolor = \"Driver Race\"\n) +\ntheme_minimal()\n\n\n\n\n\n\n\n\nThe following figure illustrates how search likelihood varies across the day and across racial groups. Certain groups show consistently higher search rates than others, even during hours where the number of stops is low.\n\nggplot(hourly, aes(x = hour, y = pct_searched, color = subject_race)) +\ngeom_line(size = 1) +\nlabs(\ntitle = \"Search Rate by Hour and Driver Race\",\nx = \"Hour of Day (0–23)\",\ny = \"Percent of Stops That Resulted in a Search\",\ncolor = \"Driver Race\"\n) +\ntheme_minimal()\n\n\n\n\n\n\n\n\nThis analysis reveals that both stop frequency and search rates vary substantially across time of day and across racial groups. Stops are most common in daytime and early evening hours, but search rates do not always follow this pattern. Instead, some racial groups experience higher search rates consistently throughout the day.\nReferences: - Pierson, E., Simoiu, C., Overgoor, J., Corbett-Davies, S., Ramachandran, V., Phillips, C., Shroff, R., & Goel, S. (2020). A large-scale analysis of racial disparities in police stops across the United States. Nature Human Behaviour. https://www.nature.com/articles/s41562-020-0858-1\n\nStanford Open Policing Project. (n.d.). Data portal. https://openpolicing.stanford.edu/data/"
  },
  {
    "objectID": "sf_rents.html",
    "href": "sf_rents.html",
    "title": "San Francisco Rents",
    "section": "",
    "text": "Original Source: Pennington, Kate (2018). Bay Area Craigslist Rental Housing Posts, 2000-2018. Retrieved from https://github.com/katepennington/historic_bay_area_craigslist_housing_posts/blob/master/clean_2000_2018.csv.zip.\nTidyTuesday Page: https://github.com/rfordatascience/tidytuesday/tree/main/data/2022/2022-07-05\n\nlibrary(tidyverse)\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-07-05')\nrent &lt;- tuesdata$rent\n\n\nggplot(rent, aes(x = price)) +\n  geom_boxplot() +\n  facet_wrap(\"county\") +\n  labs(title = \"Prices of Rentals in Different San Francisco Counties\", x = \"Price(USD)\") +\n  theme_minimal()"
  },
  {
    "objectID": "movie_genre.html",
    "href": "movie_genre.html",
    "title": "Movie Genre",
    "section": "",
    "text": "Movie ratings are widely used to evaluate films. People often claim that some genres are “better” than others based on audience scores. However, this perception might just be random variation. Understanding whether genre truly influences ratings could help studios and critics separate taste patterns from statistical noise. By examining a dataset of movies and their ratings, we can statistically test whether observed differences in ratings across genres are likely to occur by chance. A permutation test is perfect for this because it will simply test whether the association between genre and rating is stronger than we would expect if genres had no effect.\n\nlibrary(tidyverse)\n\nThe following dataset contains information on the genre and rating 10,000 different movies in the IMDb database. We are specifically interested in these variables because we want to observe the relationship between them.\n\nimdb &lt;- read_csv(\n  \"data/imdb-movies-dataset.csv\"\n) |&gt;\n  mutate(Genre2 = str_extract(Genre, \"^[A-z-]+\")) |&gt;\n  select(Title, Genre2, Rating) \n\nThe following plot shows the differences in the distributions of different genre’s movie ratings based on this data. You can see how some genres have higher ratings than others and vice versa.\n\nggplot(imdb, aes(x = Genre2, y = Rating, fill = Genre2)) +\n  geom_boxplot() +\n  labs(title = \"Rating Distribution of Movies in Different Genres\", x = \"Genre\", fill = \"Genre\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nOur research question will have the following hypotheses: - Null Hypothesis(Ho): Movie ratings are independent of genre. Any observed differences in average ratings across genres are due to random chance. - Alternative Hypothesis(Ha): Movie ratings differ by genre; some genres consistently receive higher or lower ratings.\nOur observed statistic, which measures how much genre-specific averages deviate from the overall mean rating based on the data, is as follows:\n\nimdb |&gt; summarize(avg_rating = mean(Rating, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  avg_rating\n       &lt;dbl&gt;\n1       6.44\n\nmean_rating &lt;- 6.43861\nimdb |&gt;\n  group_by(Genre2) |&gt;\n  summarize(ni = n(), xi = mean(Rating, na.rm = TRUE)) |&gt;\n  summarize(stat = sum(ni * (xi - mean_rating)^2, na.rm = TRUE))\n\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 1086.\n\nobs_stat &lt;- 1085.533\n\nNow we need a function that can simulate/permute this calculation while assuming genres are randomly assigned to movies(the null hypothesis(Ho)). In other words, each simulation randomly reassigns genres to movies to break any real association between genre and rating.\n\nimdb_sim &lt;- function(x) {\n  sim_value &lt;- imdb |&gt;\n    mutate(Genre2 = sample(Genre2, n(), replace = FALSE)) |&gt;\n    group_by(Genre2) |&gt;\n    summarize(ni = n(), xi = mean(Rating, na.rm = TRUE)) |&gt;\n    summarize(stat = sum(ni * (xi - mean_rating)^2, na.rm = TRUE)) |&gt;\n    pull(stat)\n  return(sim_value)\n}\n\nThen we can use mapping to run this simulation/permutation many times so that we can be more confident in the results.\n\nset.seed(47)\nnull_stats &lt;- map_dbl(c(1:1000), ~imdb_sim(.x))\n\nThe following plot shows the distribution of the one-thousand simulated/permuted values as well as a vertical line at the observed statistic.\n\nnull_df &lt;- data.frame(stat = null_stats)\n\nggplot(null_df, aes(x = stat)) +\n  geom_histogram(binwidth = 1, color = \"black\") +\n  geom_vline(xintercept = obs_stat, color = \"red\", linetype = \"dashed\", size = 1.2) +\n  annotate(\"text\", x = obs_stat, y = 50, label = \"Observed Statistic\", \n           color = \"red\", angle = 90, vjust = -0.5, hjust = 1.2) +\n  labs(\n    title = \"Null Distribution of Permuted Statistics\",\n    x = \"Simulated Statistic\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs you can see, not a single one of the thousand permuted statistics even came close to being at least as extreme as the observed statistic, meaning the p-value for this test is 0. This means that we have very strong evidence supporting the alternative hypthesis(Ha) that movie ratings differ by genre and that some genres consistently receive higher or lower ratings.\nReferences: Barthwal, Aman. IMDb Movies Data. Kaggle, 2024.\nhttps://www.kaggle.com/datasets/amanbarthwal/imdb-movies-data"
  },
  {
    "objectID": "freedom.html",
    "href": "freedom.html",
    "title": "Freedom",
    "section": "",
    "text": "Original Source: Freedom House & United Nations. (2022). Freedom Index / Freedom in the World dataset. Used in #TidyTuesday (2022-02-22). Freedom House; United Nations.\nTidyTuesday Page: https://github.com/rfordatascience/tidytuesday/tree/main/data/2022/2022-02-22\n\nlibrary(tidyverse)\n\n\ntuesdata &lt;- tidytuesdayR::tt_load('2022-02-22')\n\nfreedom &lt;- tuesdata$freedom\n\nfreedom &lt;- freedom |&gt; \n  mutate(status_full = case_when(Status == \"F\"  ~ \"Free\", Status == \"PF\" ~ \"Partly Free\", Status == \"NF\" ~ \"Not Free\", TRUE ~ Status))\n\n\nggplot(freedom, aes(x = CL, y = PR, color = status_full)) +\n  geom_jitter(width = 0.2, height = 0.2, alpha = 0.7) +\n  scale_color_manual(values = c(\"Free\" = \"seagreen3\", \"Partly Free\" = \"goldenrod\", \"Not Free\" = \"firebrick\")) +\n  labs(title = \"Political Rights v.s. Civil Liberties Showing Freedom Status\", x = \"Civil Liberties Score(1 = most, 7 = least)\", y = \"Political Rights Score(1 = most, 7 = least)\", color = \"Status\") +\n  theme_minimal()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Colby Cooley",
    "section": "",
    "text": "Hi there! I’m Colby. I am a student at Pomona College and I’m interested in studying data science, economics, and math. I also play for the Pomona-Pitzer football team and love hanging out with friends in my spare time. Poke around my website to learn more!"
  },
  {
    "objectID": "obama_tweets.html",
    "href": "obama_tweets.html",
    "title": "Obama Tweets",
    "section": "",
    "text": "Data from: National Archives and Records Administration. (n.d.). Obama White House social media archive: Tweets. Retrieved from Obama Federal Archives: Barack Obama Presidential Library. obamalibrary.gov\n\nlibrary(tidyverse)\n\n\nobama_tweets &lt;- read_csv(unz(\"data/POTUS111716.zip\", \"tweets.csv\"))\nobama_tweets &lt;- obama_tweets |&gt; select(text, timestamp)\n\n\nobama_tweets &lt;- obama_tweets |&gt; mutate(hashtags = str_extract_all(text, \"(?&lt;=#)\\\\w+\"))\n\n\nobama_hashtags &lt;- obama_tweets |&gt; select(hashtags) |&gt; unnest(hashtags) |&gt; group_by(hashtags) |&gt; summarize(count = n()) |&gt; arrange(desc(count)) |&gt; head(5)\n\nThe following plot shows Obama’s most used hashtags, revealing which issues he emphasized publicly. It also illustrates some of the ways that the former president used Twitter to engage with the general public.\n\nggplot(obama_hashtags, aes(x = hashtags, y = count, fill = hashtags)) +\n  geom_col(show.legend = FALSE) +\n  labs(title = \"Frequency of Obama's Top Five Most Used Twitter Hashtags\", x = \"Hashtag\", y = \"Frequency\")\n\n\n\n\n\n\n\n\n\nobama_tweets &lt;- obama_tweets |&gt; \n  mutate(text_clean = str_replace_all(text, \"https?://\\\\S+|@\\\\w+\", \"\"), text_clean = str_to_lower(text_clean))\n\n\nobama_tweets &lt;- obama_tweets |&gt; mutate(collective_count = str_count(text_clean, \"\\\\b(we|us|our|ours|we're|we've)\\\\b\"), individual_count = str_count(text_clean, \"\\\\b(i|me|my|mine|i've|i'm)\\\\b\"))\n\n\nobama_pronouns &lt;- obama_tweets |&gt; pivot_longer(c(collective_count, individual_count), names_to = \"pronoun_type\", values_to = \"count\")\n\nThe following plot shows the distribution of collective pronouns(i.e. we/us/our) in Obama’s tweets as well as the distribution of individual pronouns(i.e. I/me/my) in Obama’s tweets. It gives us insight into how Obama purposefully focused his tweets on unity and shared identity rather than on himself.\n\nggplot(obama_pronouns, aes(x = count)) +\n  facet_grid(\"pronoun_type\", labeller = labeller(pronoun_type = c(collective_count = \"Collective\",\n  individual_count = \"Individual\"))) +\n  geom_boxplot() +\n  scale_x_log10() +\n  labs(title = \"Distribution of Collective vs. Individual Pronouns in Obama's Tweets (Log Scale)\", x = \"Pronoun Count per Tweet (log scale)\")"
  },
  {
    "objectID": "taxi.html",
    "href": "taxi.html",
    "title": "NYC Taxi",
    "section": "",
    "text": "Sources\nGoodin, Dan. “Poorly Anonymized Logs Reveal NYC Cab Drivers’ Detailed Whereabouts.” Ars Technica, 27 June 2014. https://arstechnica.com/information-technology/2014/06/poorly-anonymized-logs-reveal-nyc-cab-drivers-detailed-whereabouts/.\nTockar, Anthony. “Riding with the Stars: Passenger Privacy in the NYC Taxicab Dataset.” Vet Scholar Journal Club, Kansas State University, 2024. PDF file. https://www.vet.k-state.edu/research/student-opportunities/vet-scholar/calendar/pdf/2024-journal-club-docs/session-1/Riding%20with%20the%20Stars_%20Passenger%20Privacy%20in%20the%20NYC%20Taxicab%20Dataset.pdf\n\n\nIntro\nIn 2014, as part of an open-data initiative to support transparency, entrepreneurship, and urban planning, New York City’s Taxi and Limousine Commission(TLC) publicly released data on 173 million taxi rides. The dataset included pick-up and drop-off times, locations, fares, and anonymized driver medallion numbers. Soon after, however, researchers and journalists discovered that the data could be easily de-anonymized to reveal the identities and movements of drivers and even passengers(Goodin; Tockar). The release raised deep questions about what counts as anonymized data, who decides to share it, and who bears the consequences when it’s misused.\nThe dataset was released as a structured CSV with variables to record medallion number(hashed), trip start/end timestamps, GPS coordinates, fares, and tips. But, as it turned out, the hashing used to anonymize medallion numbers was weak and could be reversed with a simple MD5 hash lookup(Goodin). Data scientists and journalists were able to re-identify by cross-referencing ride timestamps and locations with Instagram posts, paparazzi photos, and celebrity schedules(Tockar). This technique is a classic data science method used to connect separate datasets based on overlapping attributes. The case thus shows how the same analytical tools that can produce insights can also compromise privacy.\n\n\nWhat is the permission structure for using the data? Was it followed?\nThe data came from drivers and passengers and was collected under regulatory requirements for fare reporting. It was not intended for public research. Neither group was asked for consent to release detailed trip-level information(Goodin; Tockar). The NYC TLC justified release under an open data initiative, but this was institutional consent, not individual consent. It raises the question of if institutions can consent on behalf of individuals.\n\n\nIs the data identifiable? In what way? Is the data sufficiently anonymized or old to be free of ethical concerns? Is anonymity guaranteed?\nThe TLC claimed that removing names and hashing medallions ensured privacy, however, the MD5 hashes were reversible and pickup/drop-off times plus coordinates uniquely identified specific trips(Goodin). Anthony Tockar showed that a handful of coordinates could reveal celebrity movements, such as when Bradley Cooper and Jessica Alba took cabs(Tockar). The case demonstrates that anonymization is fragile when datasets are detailed but patchy. Even when they don’t have names, identity can be inferred. This links to a broader principle, the mosaic effect, which is when many harmless pieces of data combine to form identifiable pictures(Tockar).\n\n\nWas the data made publicly available? Why? How? On what platform?\nThe dataset was released on NYC’s Open Data portal for transparency and research innovation, which could potentially benefit entrepreneurs, journalists, or app developers(Goodin; Tockar). But the drivers and passengers, on the other hand, bore the privacy risk without reaping any of those benefits. This reflects a power imbalance where city officials and data scientists control data circulation. Individuals become data points in systems without being able to opt out. For example, drivers’ income and working patterns could be reconstructed, thereby invading financial privacy(Tockar).\n\n\nWhat was the consent structure for recruiting participants? Was informed consent possible? Can you provide informed consent for applications that are yet foreseen? Is the data being used in unintended ways to the original study?\nThe data were originally collected for regulatory compliance, not for open public distribution(Goodin; Tockar). No one foresaw the ways the dataset could be combined with other sources for re-identification or surveillance(Tockar). It raises the issue of whether or not we can we give informed consent for uses that haven’t been invented yet. It also demonstrates the need to anticipate future analytical capabilities(Tockar).\n\n\nConclusion\nThe NYC Taxi dataset illustrates how data science, even with good intentions such as open data or innovation, can reproduce structural power imbalances and harm those with least control over data circulation(Goodin; Tockar). Drivers lost privacy and were economically exposed, while passengers could now be tracked(Tockar). Meanwhile, data scientists and tech companies profited from free data(Goodin). The ethical violations occurred largely in the service of efficiency, transparency, and potential profit, not necessarily out of malice. This shows how claims of public benefit can obscure real harms(Tockar). The work was done by city data scientists, not affected individuals(Goodin). Researchers, startups, and journalists benefited. Taxi drivers and ordinary riders were harmed or neglected(Tockar)."
  }
]